{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow import float32, math, cast, data, equal, reduce_sum, train, function, GradientTape, argmax\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from prepare_dataset import PrepareDataset\n",
    "from transformer import Transformer\n",
    "from time import time\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n = 6\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n",
    "\n",
    "filename = 'english-german-both.pkl'\n",
    "n_sentences = 10000\n",
    "train_split = 0.8\n",
    "val_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = cast(warmup_steps, float32)\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        step_num = cast(step_num, float32)\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "\n",
    "dataset = PrepareDataset(n_sentences, train_split, val_split)\n",
    "trainX, trainY, valX, valY, train_total, val_total, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset(filename)\n",
    "\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = data.Dataset.from_tensor_slices((valX, valY))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "training_model = Transformer(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "    return reduce_sum(loss)/reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fcn(target, prediction):\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "\n",
    "    return reduce_sum(accuracy)/reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "\n",
    "val_loss = Mean(name='val_loss')\n",
    "\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, './checkpoints', max_to_keep=3)\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    print('\\nStart of the epoch', (epoch + 1))\n",
    "\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "        \n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "        if step % 50 == 0:\n",
    "            print(\"Epoch \", (epoch + 1), \" Step \", step, \": Loss \", train_loss.result(), \" Accuracy \", train_accuracy.result())\n",
    "    \n",
    "    for val_batchX, val_batchY in val_dataset:\n",
    "        encoder_input = val_batchX[:, 1:]\n",
    "        decoder_input = val_batchY[:, :-1]\n",
    "        decoder_output = val_batchY[:, 1:]\n",
    "\n",
    "        prediction = training_model(encoder_input, decoder_input, training=False)\n",
    "\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        val_loss(loss)\n",
    "\n",
    "\n",
    "    print(\"Epoch \", (epoch + 1), \": Training loss \", train_loss.result(), \" Training accuracy \", train_accuracy.result(), \" Validation loss \", val_loss.result())\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(\"Saved checkpoint at epoch: \", (epoch + 1))\n",
    "\n",
    "        training_model.save_weights('weights/wghts' + str(epoch + 1) + '.ckpt')\n",
    "\n",
    "        train_loss_dict[epoch] = train_loss.result()\n",
    "        val_loss_dict[epoch] = val_loss.result()\n",
    "\n",
    "with open('./train_loss.pkl', 'wb') as file:\n",
    "    dump(train_loss_dict, file)\n",
    "\n",
    "with open('./val_loss.pkl', 'wb') as file:\n",
    "    dump(val_loss_dict, file)\n",
    "\n",
    "print(\"Time taken: \", (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
