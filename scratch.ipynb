{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self, n_sentences, train_split, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = n_sentences\n",
    "        self.train_split = train_split\n",
    "\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = '<START> ' + dataset[i, 0] + ' <EOS>'\n",
    "            dataset[i, 1] = '<START> ' + dataset[i, 1] + ' <EOS>'\n",
    "        \n",
    "        shuffle(dataset)\n",
    "\n",
    "        train = dataset[:, :int(self.n_sentences * self.train_split)]\n",
    "\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "        \n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "\n",
    "        return (trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrepareDataset(10000, 0.9)\n",
    "trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "print('Encoder sequence length: ', enc_seq_length)\n",
    "\n",
    "print(train[0, 0], '\\n', trainX[0, :])\n",
    "\n",
    "print('Decoder sequence length: ', dec_seq_length)\n",
    "\n",
    "print(train[0, 1], '\\n', trainY[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
