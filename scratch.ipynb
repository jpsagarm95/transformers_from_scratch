{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor, string\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
    "from tensorflow.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence_length = 5\n",
    "vocab_size = 10\n",
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "sentence_data = Dataset.from_tensor_slices(sentences)\n",
    "vectorize_layer = TextVectorization(output_sequence_length=output_sequence_length, max_tokens=vocab_size)\n",
    "\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "word_tensors = convert_to_tensor(sentences, dtype=tf.string)\n",
    "\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "print(vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 6\n",
    "word_embedding_layer = Embedding(vocab_size, output_length)\n",
    "embedded_words = word_embedding_layer(vectorized_words)\n",
    "print(embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding_layer = Embedding(output_sequence_length, output_length)\n",
    "position_indices = tf.range(output_sequence_length)\n",
    "print(position_indices)\n",
    "embedded_indices = position_embedding_layer(position_indices)\n",
    "print(embedded_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_embedding = embedded_words + embedded_indices\n",
    "print(\"Final output: \", final_output_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddingLayer(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedding_layer = PositionalEmbeddingLayer(output_sequence_length, vocab_size, output_length)\n",
    "embedding_layer_output = my_embedding_layer(vectorized_words)\n",
    "print(\"Output from my_embedding_layer= \", embedding_layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddingFixedWeightsLayer(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_weights = self.get_position_encoding(vocab_size, output_dim)\n",
    "        position_embedding_weights = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim, weights=[word_embedding_weights], trainable=False)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim, weights=[position_embedding_weights], trainable=False)\n",
    "\n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i + 1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnisallyouneed_embedding = PositionalEmbeddingFixedWeightsLayer(output_sequence_length, vocab_size, output_length)\n",
    "attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\n",
    "print(\"Output of attnisallyouneed embedding layer: \", attnisallyouneed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_phrase = \"to understand machine learning algorithms you need\" +\\\n",
    "    \" to understand concepts such as gradient of a function \"+\\\n",
    "    \"Hessians of a matrix and optimization etc\"\n",
    "wise_phrase = \"patrick henry said give me liberty or give me death \"+\\\n",
    "    \"when he addressed the second virginia convention in march\"\n",
    "total_vocabulary = 200\n",
    "seq_len = 20\n",
    "final_output_len = 50\n",
    "\n",
    "phrase_vectorization_layer = TextVectorization(output_sequence_length=seq_len, max_tokens=total_vocabulary)\n",
    "phrase_vectorization_layer.adapt([technical_phrase, wise_phrase])\n",
    "\n",
    "phrase_tensors = convert_to_tensor([technical_phrase, wise_phrase], dtype=tf.string)\n",
    "\n",
    "vectorized_phrases = phrase_vectorization_layer(phrase_tensors)\n",
    "\n",
    "random_weights_embedding_layer = PositionalEmbeddingLayer(seq_length=seq_len, vocab_size=total_vocabulary, output_dim=final_output_len)\n",
    "\n",
    "fixed_weights_embedding_layer = PositionalEmbeddingFixedWeightsLayer(seq_length=seq_len, vocab_size=total_vocabulary, output_dim=final_output_len)\n",
    "\n",
    "random_embedding = random_weights_embedding_layer(vectorized_phrases)\n",
    "\n",
    "fixed_embedding = fixed_weights_embedding_layer(vectorized_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "title = [\"Tech Phrase\", \"Wise Phrase\"]\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "    matrix = random_embedding[i, :, :]\n",
    "    cax = ax.matshow(matrix)\n",
    "    plt.gcf().colorbar(cax)\n",
    "    plt.title(title[i], y=1.2)\n",
    "fig.suptitle(\"Random Embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "title = [\"Tech Phrase\", \"Wise Phrase\"]\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "    matrix = fixed_embedding[i, :, :]\n",
    "    cax = ax.matshow(matrix)\n",
    "    plt.gcf().colorbar(cax)\n",
    "    plt.title(title[i], y=1.2)\n",
    "fig.suptitle(\"Fixed Embedding\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
