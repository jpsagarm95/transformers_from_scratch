{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor, string\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
    "from tensorflow.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence_length = 5\n",
    "vocab_size = 10\n",
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "sentence_data = Dataset.from_tensor_slices(sentences)\n",
    "vectorize_layer = TextVectorization(output_sequence_length=output_sequence_length, max_tokens=vocab_size)\n",
    "\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "word_tensors = convert_to_tensor(sentences, dtype=tf.string)\n",
    "\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "print(vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = 6\n",
    "word_embedding_layer = Embedding(vocab_size, output_length)\n",
    "embedded_words = word_embedding_layer(vectorized_words)\n",
    "print(embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding_layer = Embedding(output_sequence_length, output_length)\n",
    "position_indices = tf.range(output_sequence_length)\n",
    "print(position_indices)\n",
    "embedded_indices = position_embedding_layer(position_indices)\n",
    "print(embedded_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_embedding = embedded_words + embedded_indices\n",
    "print(\"Final output: \", final_output_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddingLayer(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedding_layer = PositionalEmbeddingLayer(output_sequence_length, vocab_size, output_length)\n",
    "embedding_layer_output = my_embedding_layer(vectorized_words)\n",
    "print(\"Output from my_embedding_layer= \", embedding_layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddingFixedWeightsLayer(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_weights = self.get_position_encoding(vocab_size, output_dim)\n",
    "        position_embedding_weights = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim, weights=[word_embedding_weights], trainable=False)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim, weight=[position_embedding_weights], trainable=False)\n",
    "\n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i + 1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
