{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load, dump, HIGHEST_PROTOCOL\n",
    "from numpy.random import shuffle\n",
    "from numpy import savetxt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self, n_sentences, train_split, val_split, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = n_sentences\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return len(tokenizer.word_index) + 1\n",
    "    \n",
    "    def encode_pad(self, dataset, tokenizer, seq_length):\n",
    "        x = tokenizer.texts_to_sequences(dataset)\n",
    "        x = pad_sequences(x, maxlen=seq_length, padding='post')\n",
    "        x = convert_to_tensor(x, dtype=int64)\n",
    "        return x\n",
    "\n",
    "    def save_tokenizer(self, tokenizer, name):\n",
    "        with open(name + '_tokenizer.pkl', 'wb') as handle:\n",
    "            dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = '<START> ' + dataset[i, 0] + ' <EOS>'\n",
    "            dataset[i, 1] = '<START> ' + dataset[i, 1] + ' <EOS>'\n",
    "        \n",
    "        shuffle(dataset)\n",
    "\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "        val = dataset[int(self.n_sentences * self.train_split): int(self.n_sentences * (1 - self.val_split))]\n",
    "        test = dataset[int(self.n_sentences * (1 - self.val_split)):]\n",
    "\n",
    "        enc_tokenizer = self.create_tokenizer(dataset[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(dataset[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(dataset[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n",
    "        trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n",
    "\n",
    "        valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n",
    "        valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n",
    "\n",
    "        self.save_tokenizer(enc_tokenizer, 'enc')\n",
    "        self.save_tokenizer(dec_tokenizer, 'dec')\n",
    "\n",
    "        savetxt('test_dataset.txt', test, fmt='%s')\n",
    "\n",
    "        return (trainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrepareDataset(10000, 0.8, 0.1)\n",
    "trainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "print('Training data size: ', shape(train))\n",
    "print('Validation data size: ', shape(val))\n",
    "\n",
    "print('Encoder sequence length: ', enc_seq_length)\n",
    "\n",
    "print(train[0, 0], '\\n', trainX[0, :])\n",
    "\n",
    "print('Decoder sequence length: ', dec_seq_length)\n",
    "\n",
    "print(train[0, 1], '\\n', trainY[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
